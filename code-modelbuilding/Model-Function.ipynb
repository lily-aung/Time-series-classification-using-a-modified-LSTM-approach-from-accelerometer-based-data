{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model to detect Heel Strike and Toe-off events\n",
    "\n",
    "In this notebook, we will predict whether each sample is a heel strike or toe-off based on the accelerometer data. To do so, we train an LSTM (Long-Short Term Memory) model on 70% of the data and test it on the remaining 30%\n",
    "\n",
    "The LSTM architecture has\n",
    "- a lookback window of 3 timesteps\n",
    "- 6 features (X,Y,Z acceleration of left foot and right foot) \n",
    "- 44 hidden nodes\n",
    "\n",
    "**The code is the same as that in Model1.pynb, except that we use a for loop to iterate over all possible combinations of SUBJECT_ID, ACTIVITY and EVENT**\n",
    "\n",
    "The subsequent steps to tune and test the model are as follows:\n",
    "\n",
    "1. Subset the data based on subject, activity, and event\n",
    "2. Remove noise from the data\n",
    "3. Re-scale each of the features to range (0,5)\n",
    "4. Split the data into training and testing sets  \n",
    "5. Since we are using an LSTM architecture, we will implement a look-back function that will introduce historical data determined by the window size\n",
    "6. Build and compile the Neural Network\n",
    "7. Fit the model\n",
    "8. Save the predictions and actual value to evaluate the model. The model is evaluated in terms of \n",
    "    - F1 score\n",
    "    - Percentage of true positives\n",
    "    - Mean of the absolute difference in time between true positives and corresponding GT events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tankh/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: before you run the code\n",
    "\n",
    "You can adjust the list of subject Ids, activities and events in the next cell.\n",
    "Some parts are specific to indoors/outdoors, so you will need to comment the part if needed, and uncomment it it's if not needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Specify the list of subjects, activities and events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Select Subject Id #######\n",
    "# Key in a number from 1 to 20. \n",
    "\n",
    "# indoors\n",
    "SUBJECT_ID_LIST = ['2']\n",
    "# SUBJECT_ID_LIST = ['1','2','3','4','5','6','7','8','9','10','11']\n",
    "# # outdoors\n",
    "# SUBJECT_ID_LIST = ['12','13','14','15','16','17','18','19','20']\n",
    "\n",
    "###### Select Activity ######\n",
    "# Key in one of the options below:\n",
    "\n",
    "# indoors\n",
    "ACTIVITY_LIST = ['indoor_walk','indoor_run','indoor_walknrun']\n",
    "# ACTIVITY_LIST = ['treadmill_walk','treadmill_walknrun','treadmill_run','treadmill_all','indoor_walk','indoor_run','indoor_walknrun']\n",
    "# # outdoors\n",
    "# ACTIVITY_LIST = ['outdoor_walk','outdoor_run','outdoor_walknrun']\n",
    "\n",
    "###### Select Event ######\n",
    "EVENT_LIST = ['LF_HS','LF_TO','RF_HS','RF_TO']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define helper functions\n",
    "\n",
    "We will define two functions to be used in the for loop later, one to create a indicator variable for 'treadmill_all', the other to create the dataset for feeding into the RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is no indicator variable for treadmill_all for subjects 1-11, let's create another column which for this. The value of this data is 1 if either treadmill_walknrun is 1 or treadmill_slope_walk is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "######## Specific to indoors only #############\n",
    "###############################################\n",
    "\n",
    "# for SUBJECT_ID 1 to 11, uncomment if otherwise\n",
    "# we will use this function in the for loop later\n",
    "\n",
    "def label_treadmill_all (row):\n",
    "   if row['treadmill_walknrun'] == 1:\n",
    "      return 1\n",
    "   if row['treadmill_slope_walk'] == 1:\n",
    "      return 1\n",
    "   return 0\n",
    "\n",
    "def label_treadmill_run (row):\n",
    "   if row['treadmill_walknrun'] == 1 and row['treadmill_walk'] == 0:\n",
    "      return 1\n",
    "   return 0\n",
    "\n",
    "def label_indoor_run (row):\n",
    "   if row['indoor_walknrun'] == 1 and row['indoor_walk'] == 0:\n",
    "      return 1\n",
    "   return 0\n",
    "\n",
    "###############################################\n",
    "######## Specific to outdoors only ############\n",
    "###############################################\n",
    "\n",
    "# for SUBJECT_ID 12 to 20, uncomment if otherwise\n",
    "# we will use this function in the for loop later\n",
    "\n",
    "def label_outdoor_run (row):\n",
    "   if row['outdoor_walknrun'] == 1 and row['outdoor_walk'] == 0:\n",
    "      return 1\n",
    "   return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "# we will use this function in the for loop later\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back-1):\n",
    "\t\ta = dataset[i:(i+look_back), 0:6]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, 6])\n",
    "\treturn numpy.array(dataX), numpy.array(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Model and save Predictions\n",
    "\n",
    "If training for indoor activity, use the next cell and comment the cell after that.\n",
    "\n",
    "If training for indoor activity, comment the next cell and use the cell after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for SUBJECT_ID in SUBJECT_ID_LIST:\n",
    "    for ACTIVITY in ACTIVITY_LIST:\n",
    "        for EVENT in EVENT_LIST:\n",
    "            # read in the data\n",
    "            DATA_PATH = './Combined Data_csv format/'\n",
    "            df = pd.read_csv(DATA_PATH + 'Sub_'+ SUBJECT_ID + '.csv', header = 0)          \n",
    "            df = df.drop(df.columns[0], axis=1)\n",
    "            df['treadmill_all']=df.apply (lambda row: label_treadmill_all(row),axis=1)\n",
    "            df['treadmill_run']=df.apply (lambda row: label_treadmill_run(row),axis=1)\n",
    "            df['indoor_run']=df.apply (lambda row: label_indoor_run(row),axis=1)\n",
    "            \n",
    "            print(\"Currently running for: \" + \" -Subject: \" + SUBJECT_ID + ' -Activity: ' + ACTIVITY + ' -Event: ' + EVENT)\n",
    "\n",
    "            # Subset out the data by activity of interest\n",
    "            k1=df[df[ACTIVITY]==1]\n",
    "            k2 = k1[['accX_LF','accY_LF','accZ_LF','accX_RF','accY_RF','accZ_RF', EVENT]]\n",
    "            \n",
    "            # Remove noise from data\n",
    "            k2['accX_LF_median']=k2['accX_LF'].rolling(window=3).mean()\n",
    "            k2['accY_LF_median']=k2['accY_LF'].rolling(window=3).mean()\n",
    "            k2['accZ_LF_median']=k2['accZ_LF'].rolling(window=3).mean()\n",
    "            k2['accX_RF_median']=k2['accX_RF'].rolling(window=3).mean()\n",
    "            k2['accY_RF_median']=k2['accY_RF'].rolling(window=3).mean()\n",
    "            k2['accZ_RF_median']=k2['accZ_RF'].rolling(window=3).mean()\n",
    "\n",
    "            k3 = k2[['accX_LF_median','accY_LF_median','accZ_LF_median','accX_RF_median','accY_RF_median','accZ_RF_median', EVENT]]\n",
    "            k3 = k3.iloc[2:]\n",
    "            dataset = k3.values\n",
    "\n",
    "            # normalize the dataset\n",
    "            scaler = MinMaxScaler(feature_range=(0, 5))\n",
    "            dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "            # split into train and test sets\n",
    "            train_size = int(len(dataset) * 0.7)\n",
    "            test_size = len(dataset) - train_size\n",
    "            train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "\n",
    "            look_back=3 # 3 timesteps\n",
    "            trainX, trainY = create_dataset(train, look_back)\n",
    "            testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "            model = Sequential()\n",
    "\n",
    "            # Recurrent layer\n",
    "            model.add(LSTM(44, return_sequences=False, dropout=0.1, recurrent_dropout=0.1))\n",
    "            # Fully connected layer\n",
    "            model.add(Dense(44, activation='relu'))\n",
    "            # Dropout for regularization\n",
    "            model.add(Dropout(0.5))\n",
    "            # Output layer\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "            # Compile the model\n",
    "            model.compile(\n",
    "                optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "            # Fit the model\n",
    "            es= EarlyStopping(monitor='val_loss', min_delta=0,patience=5,verbose=0, mode='auto')\n",
    "            history = model.fit(trainX, trainY, validation_split = 0.33, epochs=50, batch_size=20, verbose=0, callbacks=[TQDMNotebookCallback(),es])\n",
    "\n",
    "            # List all data in history\n",
    "            print(history.history.keys())\n",
    "\n",
    "            # summarize history for accuracy\n",
    "            plt.plot(history.history['acc'])\n",
    "            plt.plot(history.history['val_acc'])\n",
    "            plt.title('model accuracy')\n",
    "            plt.ylabel('accuracy')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train','validation'])\n",
    "            plt.show()\n",
    "\n",
    "            # summarize history for loss\n",
    "            plt.plot(history.history['loss'])\n",
    "            plt.plot(history.history['val_loss'])\n",
    "            plt.title('model loss')\n",
    "            plt.ylabel('loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train','validation'])\n",
    "            plt.show()\n",
    "\n",
    "            model.summary()\n",
    "\n",
    "            testY_predict = model.predict(testX)\n",
    "\n",
    "            predicted = []\n",
    "            actual = []\n",
    "            for i in range(len(testY)):\n",
    "                #print(int(testY_predict[i]>0.5), int(testY[i]>0.5))\n",
    "                predicted.append(int(testY_predict[i]>0.5))\n",
    "                actual.append(int(testY[i]>0.5))\n",
    "\n",
    "            PREDICTIONS_DF = pd.DataFrame(list(zip(predicted, actual)), columns=['predicted','actual'])\n",
    "\n",
    "            # write dataframe to .csv\n",
    "            SAVE_PATH = './Predicted Data_model_csv format/'\n",
    "            PREDICTIONS_DF.to_csv(SAVE_PATH+'Sub'+SUBJECT_ID+'_'+ ACTIVITY + '_'+ EVENT + '.csv', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently running for:  -Subject: 18 -Activity: outdoor_walknrun -Event: RF_HS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tankh/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/tankh/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/tankh/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/tankh/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/tankh/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/tankh/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e548081491f4c20a01abd0be123f978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=50, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a720d99ed14990b9bf8a7e8cbecd40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0', max=22883, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5b6b17287528>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mes\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.33\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTQDMNotebookCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;31m# List all data in history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2669\u001b[0m                                 \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2671\u001b[0;31m                                 session)\n\u001b[0m\u001b[1;32m   2672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_make_callable\u001b[0;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[1;32m   2621\u001b[0m             \u001b[0mcallable_opts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2622\u001b[0m         \u001b[0;31m# Create callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2623\u001b[0;31m         \u001b[0mcallable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2624\u001b[0m         \u001b[0;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2625\u001b[0m         \u001b[0;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[0;34m(self, callable_options)\u001b[0m\n\u001b[1;32m   1469\u001b[0m     \"\"\"\n\u001b[1;32m   1470\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1471\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session, callable_options)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m           self._handle = tf_session.TF_SessionMakeCallable(\n\u001b[0;32m-> 1425\u001b[0;31m               session._session, options_ptr, status)\n\u001b[0m\u001b[1;32m   1426\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m         \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# for SUBJECT_ID in SUBJECT_ID_LIST:\n",
    "#     for ACTIVITY in ACTIVITY_LIST:\n",
    "#         for EVENT in EVENT_LIST:\n",
    "#             # read in the data\n",
    "#             DATA_PATH = './Combined Data_csv format/'\n",
    "#             df = pd.read_csv(DATA_PATH + 'Sub_'+ SUBJECT_ID + '.csv', header = 0)          \n",
    "#             df = df.drop(df.columns[0], axis=1)\n",
    "#             df['outdoor_run']=df.apply (lambda row: label_outdoor_run(row),axis=1)\n",
    "            \n",
    "#             print(\"Currently running for: \" + \" -Subject: \" + SUBJECT_ID + ' -Activity: ' + ACTIVITY + ' -Event: ' + EVENT)\n",
    "\n",
    "#             # Subset out the data by activity of interest\n",
    "#             k1=df[df[ACTIVITY]==1]\n",
    "#             k2 = k1[['accX_LF','accY_LF','accZ_LF','accX_RF','accY_RF','accZ_RF', EVENT]]\n",
    "            \n",
    "#             # Remove noise from data\n",
    "#             k2['accX_LF_median']=k2['accX_LF'].rolling(window=3).mean()\n",
    "#             k2['accY_LF_median']=k2['accY_LF'].rolling(window=3).mean()\n",
    "#             k2['accZ_LF_median']=k2['accZ_LF'].rolling(window=3).mean()\n",
    "#             k2['accX_RF_median']=k2['accX_RF'].rolling(window=3).mean()\n",
    "#             k2['accY_RF_median']=k2['accY_RF'].rolling(window=3).mean()\n",
    "#             k2['accZ_RF_median']=k2['accZ_RF'].rolling(window=3).mean()\n",
    "\n",
    "#             k3 = k2[['accX_LF_median','accY_LF_median','accZ_LF_median','accX_RF_median','accY_RF_median','accZ_RF_median', EVENT]]\n",
    "#             k3 = k3.iloc[2:]\n",
    "#             dataset = k3.values\n",
    "\n",
    "#             # normalize the dataset\n",
    "#             scaler = MinMaxScaler(feature_range=(0, 5))\n",
    "#             dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "#             # split into train and test sets\n",
    "#             train_size = int(len(dataset) * 0.7)\n",
    "#             test_size = len(dataset) - train_size\n",
    "#             train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "\n",
    "#             look_back=3 # 3 timesteps\n",
    "#             trainX, trainY = create_dataset(train, look_back)\n",
    "#             testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "#             model = Sequential()\n",
    "\n",
    "#             # Recurrent layer\n",
    "#             model.add(LSTM(44, return_sequences=False, dropout=0.1, recurrent_dropout=0.1))\n",
    "#             # Fully connected layer\n",
    "#             model.add(Dense(44, activation='relu'))\n",
    "#             # Dropout for regularization\n",
    "#             model.add(Dropout(0.5))\n",
    "#             # Output layer\n",
    "#             model.add(Dense(1, activation='sigmoid'))\n",
    "#             # Compile the model\n",
    "#             model.compile(\n",
    "#                 optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#             # Fit the model\n",
    "#             es= EarlyStopping(monitor='val_loss', min_delta=0,patience=5,verbose=0, mode='auto')\n",
    "#             history = model.fit(trainX, trainY, validation_split = 0.33, epochs=50, batch_size=20, verbose=0, callbacks=[TQDMNotebookCallback(),es])\n",
    "\n",
    "#             # List all data in history\n",
    "#             print(history.history.keys())\n",
    "\n",
    "#             # summarize history for accuracy\n",
    "#             plt.plot(history.history['acc'])\n",
    "#             plt.plot(history.history['val_acc'])\n",
    "#             plt.title('model accuracy')\n",
    "#             plt.ylabel('accuracy')\n",
    "#             plt.xlabel('epoch')\n",
    "#             plt.legend(['train','validation'])\n",
    "#             plt.show()\n",
    "\n",
    "#             # summarize history for loss\n",
    "#             plt.plot(history.history['loss'])\n",
    "#             plt.plot(history.history['val_loss'])\n",
    "#             plt.title('model loss')\n",
    "#             plt.ylabel('loss')\n",
    "#             plt.xlabel('epoch')\n",
    "#             plt.legend(['train','validation'])\n",
    "#             plt.show()\n",
    "\n",
    "#             model.summary()\n",
    "\n",
    "#             testY_predict = model.predict(testX)\n",
    "\n",
    "#             predicted = []\n",
    "#             actual = []\n",
    "#             for i in range(len(testY)):\n",
    "#                 #print(int(testY_predict[i]>0.5), int(testY[i]>0.5))\n",
    "#                 predicted.append(int(testY_predict[i]>0.5))\n",
    "#                 actual.append(int(testY[i]>0.5))\n",
    "\n",
    "#             PREDICTIONS_DF = pd.DataFrame(list(zip(predicted, actual)), columns=['predicted','actual'])\n",
    "\n",
    "#             # write dataframe to .csv\n",
    "#             SAVE_PATH = './Predicted Data_model_csv format/'\n",
    "#             PREDICTIONS_DF.to_csv(SAVE_PATH+'Sub'+SUBJECT_ID+'_'+ ACTIVITY + '_'+ EVENT + '.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
